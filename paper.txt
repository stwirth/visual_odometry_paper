.. Info:
   column width is 252pt
   text width is 516pt
   where 1 inch = 72.27pt

.. role:: raw-tex(raw)
  :format: latex html

.. role:: raw-math(raw)
  :format: latex html

.. role:: ref

.. role:: label

.. raw:: latex

  \newcommand*{\docutilsroleref}{\ref}
  \newcommand*{\docutilsrolelabel}{\label}


Visual Odometry for Autonomous Underwater Vehicles
==================================================
| *Stephan Wirth*
| *Universitat de les Illes Balears*
| *stephan.wirth@uib.es*

| *Pep Llu√≠s Negre Carrasco*
| *Universitat de les Illes Balears*
| *pepllu.negre@gmail.com*

| *Gabriel Oliver Codina*
| *Universitat de les Illes Balears*
| *goliver@uib.cat*


Introduction
------------
Vision based localization, mapping and motion estimation algorithms are widely used in ground-based and aerial robotics. They have proven to be a precise and low-cost alternative to inertial measurement units and laser scanners. In this paper we show that stereo vision based odometry can be used by autonomous underwater vehicles (AUVs) that navigate close to the seabed for incremental pose estimation and 3D reconstruction of small areas. We present the integration of a stereo visual odometry system into an AUV and experiments carried out in laboratory and harbour conditions comparing vision based pose estimates with ground truth.

Visual Odometry System
----------------------
The first choice to make for setting up a visual odometry system is the type of cameras to use. For a stereo camera system, one can choose between a fixed system that contains two cameras in one case and a custom made system of two single cameras. While the latter is more flexible as one can adapt the baseline the former is easier to integrate as synchronization and alignment are optimized by the manufacturer.
We use a standard stereo camera with a custom waterproof housing.

The visual odometry software we choose is based on *libviso2* :raw-tex:`\cite{Geiger2011}`. The processing pipeline contains five consecutive steps:

1. Image Acquisition
2. Preprocessing
3. Feature Extraction
4. Feature Matching
5. Motion Estimation

*libviso2* covers steps 3 to 5. We added an open source wrapper for this library [#]_ to integrate it into the Robot Operating System ROS :raw-tex:`\cite{Quigley2009}`. This simplifies the integration in different robotic vehicles as well as comparison with other integrated sensors. For step 2 we use the OpenCV library :raw-tex:`\cite{Bradski2000}` functions that are integrated in ROS and step 1 is done by a custom stereo camera driver.

.. [#] See http://www.ros.org/viso2_ros for source code and documentation.

.. Image Acquisition

.. The design of the stereo system determines the precision of the overall system. Generally speaking, higher resolution, greater field of view and greater baseline improve precision. However, increasing resolution increases processing time as well, greater field of view reduces details and a greater baseline can lead to matching problems as the scene may look different from left and right camera positions. We formally present these variables and the dependencies on the environment.

.. Preprocessing

.. The first step of working with stereo images is undistortion and rectification. After this process, corresponding points of left and right images lie on the same line.

.. Feature Extraction

.. Feature extraction is done by applying blob and corner detection masks on the image. The result is a huge amount of features of four classes. Non-minimum and non-maximum suppression is used for filtering.

..  Matching

.. The feature matching 

..  Motion Estimation

Evaluation Method
-----------------

.. possibly add reference to KITTI benchmark here

As visual odometry suffers from drift, comparing whole trajectories, i.e. poses in time directly to ground truth is not very meaningful. Instead, we subdivide the path into small pieces and compare relative transformations for each piece to the matching piece in our ground truth. We evaluate both translation and rotation errors.

Experimental Setup
------------------
.. figure:: images/G500-Experiments.jpg
  :width: 100 % 

  :label:`g500` The Girona 500 AUV during laboratory and harbour experiments. The white box in the front of the lower part is the stereo camera. The cylinder right next to it contains the image processing unit.

The experiments carried out use data gathered during the TRIDENT project [#]_ both in laboratory and harbour conditions using the Girona500 AUV :raw-tex:`\cite{Ribas2012}` as vehicle (Figure :ref:`g500`). Figure :ref:`sample-images` shows some example images from both sequences. Ground truth for the laboratory experiment is extracted by matching each image that has been captured against the known image that is printed on the floor of the test pool. As the size of the print is known, 6 DOF camera poses can be computed minimizing feature reprojection errors.

.. figure:: images/sample_images.jpg
  :width: 100 % 

  :label:`sample-images` Sample images from the sequences during the laboratory (top row) and harbour (bottom row) experiments.

For the harbour experiment, the determination of ground truth is more difficult, as no external sensors have been used and positions of natural landmarks are not known. One aim of the project was the construction of a consistent seabed mosaic. The computation of this mosaic includes global optimization of all camera poses. The resulting trajectory does not suffer from drift and is therefore chosen as our ground truth reference.

.. [#] see http://www.irs.uji.es/trident/

In the harbour experiment, a DVL and an AHS have been used as navigation sensors to let the vehicle follow a previously defined trajectory autonomously. In the laboratory experiments, the vehicle has been remotely controlled by a human operator. Table :ref:`experiment-details` summarizes the characteristics of the experiments.
The AUV was configured to not control pitch, roll and sway. Motions in these degrees of freedom are therefore rather small and we cannot compute reasonable error measures for them.

.. raw:: latex

  \begin{table}[h]
  \begin{tabular}{l*{3}{c}r}
  & \textbf{CIRS Lab} & \textbf{Roses Harbour}  \\
  \hline
  Total Trajectory Length (m)     & 47.54 & 90.48  \\[3pt]
  Average Velocity (m/s)          & 0.15 & 0.12  \\[3pt]
  Average Altitude (m)            & 1.59 & 1.34  \\[3pt]
  Average Depth (m)               & 2.98 & 1.46  \\[3pt]
  \end{tabular}
  \caption{\DUrole{label}{experiment-details} Experiment details}
  \end{table}


Results
-------

A comparison of linear and angular velocity estimates to ground truth can be found in Figure ??. Table :ref:`error-meas` gives detailed error measurements.

.. figure:: aux/cirs-timefixed-run3-vyaw.pdf
  :width: 100 % 

  :label:`vyaw-cirs` Angular velocity about the vehicle's vertical axis (yaw) for the laboratory experiment.

.. figure:: aux/roses-vyaw.pdf
  :width: 100 % 

  :label:`vyaw-roses` Angular velocity about the vehicle's vertical axis (yaw) for the harbour experiment.

.. figure:: aux/cirs-timefixed-run3-vt.pdf
  :width: 100 % 

  :label:`vt-cirs` Linear velocity during the laboratory experiment.

.. figure:: aux/roses-vt.pdf
  :width: 100 % 

  :label:`vt-roses` Linear velocitiy during the harbour experiment.



.. CIRS Lab
.. ^^^^^^^^

.. Roses Harbour
.. ^^^^^^^^^^^^^

.. 3D Reconstruction
.. ^^^^^^^^^^^^^^^^^


.. raw:: latex

  \begin{table}[h]
  \begin{tabular}{l*{3}{c}r}
  & \textbf{CIRS Lab} & \textbf{Roses Harbour}  \\
  \hline
  Translation Error (m)       & 0.056 & 0.471  \\[3pt]
  Rotational Yaw Error (m/s)  & 0.003 & 0.001  \\[3pt]
  \end{tabular}
  \caption{\DUrole{label}{error-meas} Average absolute errors in velocities.}
  \end{table}


.. TODO for full paper: create nice 3D views of reconstructed tire from roses (and blackbox from cirs?)


Conclusion
----------

Visual odometry is a method for motion estimation that gives good results using a low-cost sensor in reasonable conditions. We have shown that it can be used in an underwater environment if both the visibility conditions and the appearance of the sea floor result in images with sufficient texture. Like all incremental motion estimation methods, visual odometry suffers from drift and has to be combined with other sensors to get precise long-term position estimates. Apart from that, failure might occur in situations with insufficient texture, motion blur, or bad visibility. One important feature of the system is that it detects the failures and can signal them to higher level processing units.

Despite the mentioned drawbacks we have shown that a visual odometer using a downward looking stereo camera can be a valuable sensor for underwater vehicles giving good estimates for linear movement and rotation about the vehicle's vertical axis. We have shown that the precision and speed of the presented system is sufficient for the reconstruction and visualization of small 3D environments.

.. raw:: latex

  \bibliographystyle{plain}
  \bibliography{references}
