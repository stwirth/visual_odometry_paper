.. Info:
   column width is 252pt
   text width is 516pt
   where 1 inch = 72.27pt

.. role:: raw-tex(raw)
  :format: latex html

.. role:: raw-math(raw)
  :format: latex html

.. role:: ref

.. role:: label

.. raw:: latex

  \newcommand*{\docutilsroleref}{\ref}
  \newcommand*{\docutilsrolelabel}{\label}


Visual Odometry for Autonomous Underwater Vehicles
==================================================

| *Stephan Wirth, Pep Lluis Negre Carrasco, Gabriel Oliver Codina*
| *Universitat de les Illes Balears*
| *stephan.wirth@uib.es, pepllu.negre@gmail.com, goliver@uib.cat*


Abstract
--------
Vision based motion estimation algorithms are widely used in ground-based and aerial robotics. Combined with inertial measurement units, they have proven to be a precise and low-cost sensor for velocity and pose estimation. In this paper we show that stereo vision based odometry can be used by autonomous underwater vehicles (AUVs) that navigate close to the seabed for incremental pose estimation and 3D reconstruction of small areas. We present the integration of two different stereo visual odometry algorithms into an AUV and experiments carried out in laboratory and harbour conditions comparing vision based pose estimates with ground truth.
.. and DVL measurements?

Introduction
------------
TODO Related work
(Any publications on underwater visual odometry (including AUVs), else UAV or automobile systems)

No GPS underwater

DVL very expensive, vision+IMU cheap

Underwater vision dificulties

Visual Odometry Systems
-----------------------
A visual odometry system consists of one or more cameras, a processing unit, and the required algorithms that process incoming images. 
Algorithms for visual odometry - as opposed to full SLAM algorithms - focus on fast frame-to-frame motion estimates without keeping a large history for loop closing. The emphasis lies on accurate measurements at high frequencies. Systems that use just one camera need translational movement for 3D motion estimation and all measurements have to be scaled by an unknown factor to be on a metric scale. Using a calibrated stereo camera overcomes both of the aforementioned problems as 3D coordinates of matched points in a single left/right image pair can be computed by triangulation. We therefore focus our work on visual odometry systems that use stereo cameras.

The two publicly available algorithms we compare are *libviso2* :raw-tex:`\cite{Geiger2011}` and *fovis* :raw-tex:`\cite{Huang2011}`. The former has been successfully used on cars, and the latter has proven to work for micro aerial vehicles. 

Both algorithms have very similar processing pipelines containing feature detection, filtering, matching, and motion estimation. Table :ref:`odometry-comparison` summarizes the steps of the algorithms, outlining similarities and differences. To limit the extend of this paper, the interested reader is refered to the original papers for a deeper understanding and more detailed description of the algorithms. 

We created wrappers for both libraries [#]_ to integrate them into the Robot Operating System ROS :raw-tex:`\cite{Quigley2009}`. This simplifies the integration in different robotic vehicles as well as comparison with other integrated sensors.

.. [#] See http://www.ros.org/viso2_ros and http://www.ros.org/fovis_ros for source code and documentation.

.. raw:: latex

  \begin{table*}[h]
    \begin{tabularx}{\textwidth}{l|X|X}
      & \textbf{libviso2} & \textbf{fovis}  \\
      \hline
      Feature Scales                  & 2 levels, interpolated & 3 gaussian pyramid levels \\[3pt]
      Feature Detection               & 5x5 blob and corner masks & FAST with adaptive threshold  \\[3pt]
      Initial Feature Reduction       & non-minimum-/non-maximum-suppression & grid filter \\[3pt]
      Descriptor                      & 16 Sobel filter responses & 9x9 pixel window, intensity normalized \\[3pt]
      Subpixel Position Refinement    & parabolic fitting & ESM \\[3pt]
      \hline
      Matching Search Space Limitation & two-step candidate reduction & initial rotation estimate (Mei et al.) + search window \\[3pt]
      Matching Images                 & circlular match & left to right \& left to previous left \\[3pt]
      Match Reduction                 & grid filter & none \\[3pt]
      \hline
      Initial Outlier Rejection       & 2D neighborhood support & none \\[3pt]
      Outlier Classification          & iteratively during motion estimation (RANSAC) & maximally consistent clique \\[3pt]
      Minimized Reprojection Error    & previous 3D to current left and right image & previous 3D to current left and current 3D to previous left image \\[3pt]
      \hline
      Motion Filtering                & Kalman & None (external) \\[3pt]
      Keyframe Selection              & none (external) & automatic, based on number of matches \\[3pt]
    \end{tabularx}
    \caption{\DUrole{label}{odometry-comparison} Comparison of the two visual odometry algorithms.}
  \end{table*}


Evaluation Method
-----------------

.. add reference to KITTI benchmark here?

As visual odometry suffers from drift, comparing whole trajectories, i.e. poses in time directly to ground truth is not very meaningful. Instead, we subdivide the path into small pieces and compare velocities for each piece to the matching piece in our ground truth. We evaluate both translation and rotation errors.

.. raw:: latex

  \begin{figure*}
  \begin{center}
  \subfloat[]{
  \begin{tabular}{c}
  \includegraphics[width=0.45\textwidth]{aux/cirs-timefixed-run3-vt.pdf}\\
  \includegraphics[width=0.45\textwidth]{aux/roses-vt.pdf}
  \end{tabular}
  }
  \subfloat[]{
  \begin{tabular}{c}
  \includegraphics[width=0.45\textwidth]{aux/cirs-timefixed-run3-vyaw.pdf}\\
  \includegraphics[width=0.45\textwidth]{aux/roses-vyaw.pdf}
  \end{tabular}
  }
  \end{center}
  \caption{Linear (a) and angular (b) velocity estimates compared to ground truth for the laboratory (top) and the harbour (bottom) experiments.}
  \label{error-plots}
  \end{figure*}


Experiments
-----------

TODO describe used hardware (cameras & CPU)

.. figure:: images/G500-Experiments.jpg
  :width: 100 % 

  :label:`g500` The Girona 500 AUV during laboratory and harbour experiments. The white box in the front of the lower part is the stereo camera. The cylinder right next to it contains the image processing unit.

The experiments carried out use data gathered during the TRIDENT project [#]_ both in laboratory and harbour conditions using the Girona500 AUV :raw-tex:`\cite{Ribas2012}` as vehicle (Figure :ref:`g500`). Figure :ref:`sample-images` shows some example images from both sequences. Ground truth for the laboratory experiment is extracted by matching each image that has been captured against the known image that is printed on the floor of the test pool. As the size of the print is known, 6 DOF camera poses can be computed minimizing feature reprojection errors.

.. figure:: images/sample_images.jpg
  :width: 100 % 

  :label:`sample-images` Sample images from the sequences during the laboratory (top row) and harbour (bottom row) experiments.

For the harbour experiment, the determination of ground truth is more difficult, as no external sensors have been used and positions of natural landmarks are not known. One aim of the project was the construction of a consistent seabed mosaic (see :raw-tex:`\cite{Ferrer2007}`). The computation of this mosaic includes global optimization of all camera poses. The resulting trajectory does not suffer from drift and is therefore chosen as our ground truth reference.

.. [#] see http://www.irs.uji.es/trident/

In the harbour experiment, a DVL and an AHS have been used as navigation sensors to let the vehicle follow a previously defined trajectory autonomously. In the laboratory experiments, the vehicle has been remotely controlled by a human operator. Table :ref:`experiment-details` summarizes the characteristics of the experiments.

The AUV was configured to not control pitch, roll and sway. Motions in these degrees of freedom are therefore rather small and we cannot compute reasonable error measures for them.

.. TODO: Update plots to contain output from both odometry algorithms with (manually) optimized parameters. Add the plot of the trajectory?

A comparison of linear velocity estimates to ground truth can be found in Figure :ref:`error-plots`. Note that the estimated angular velocities hardly differ from the ground truth throughout the whole path.

.. raw:: latex

  \begin{table}[h]
  \begin{tabular}{l*{3}{c}r}
  & \textbf{CIRS Lab} & \textbf{Roses Harbour}  \\
  \hline
  Total Trajectory Length (m)     & 47.54 & 90.48  \\[3pt]
  Average Velocity (m/s)          & 0.15 & 0.12  \\[3pt]
  Average Altitude (m)            & 1.59 & 1.34  \\[3pt]
  Average Depth (m)               & 2.98 & 1.46  \\[3pt]
  \end{tabular}
  \caption{\DUrole{label}{experiment-details} Experiment details}
  \end{table}


Conclusion
----------

Visual odometry is a method for motion estimation that gives good results using a low-cost sensor in reasonable conditions. We presented open source wrappers for two publicly available visual odometry algorithms to the community that ease the integration into existing robotic platforms.
We have shown that these algorithms can be used in an underwater environment if both the visibility conditions and the appearance of the sea floor result in images with sufficient texture. Like all incremental motion estimation methods, visual odometry suffers from drift and has to be combined with other sensors to get precise long-term position estimates. Apart from that, failure might occur in situations with insufficient texture, motion blur, or bad visibility. 

Despite the mentioned drawbacks we have shown that a visual odometer using a downward looking stereo camera can be a valuable sensor for underwater vehicles giving good estimates for linear movement and rotation about the vehicle's vertical axis. 
TODO: Complete with experiment results.

.. raw:: latex

  \bibliographystyle{plain}
  \bibliography{references}
