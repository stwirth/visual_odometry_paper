
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}

% CITATION PACKAGES
\usepackage{cite}

% GRAPHICS RELATED PACKAGES
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  \graphicspath{{aux/}}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  \usepackage[dvips]{graphicx}
  \graphicspath{{aux/}}
  \DeclareGraphicsExtensions{.eps}
\fi

% MATH PACKAGES
\usepackage[cmex10]{amsmath}
\interdisplaylinepenalty=2500

% SPECIALIZED LIST PACKAGES
\usepackage{algorithmic}

% ALIGNMENT PACKAGES
\usepackage{array}
\usepackage{mdwmath}
\usepackage{mdwtab}
\usepackage{eqparbox}

% SUBFIGURE PACKAGES
\usepackage[caption=false,font=footnotesize]{subfig}
%\usepackage[tight,footnotesize]{subfigure}
\usepackage{multirow,tabularx}

% FLOAT PACKAGES
\usepackage{fixltx2e}
\usepackage{stfloats}
\usepackage{float}

% PDF, URL AND HYPERLINK PACKAGES
\usepackage{url}

% fix search and cut-and-paste in Acrobat
\usepackage{cmap}
\usepackage{ifthen}

% place figures here definitely
%\floatplacement{figure}{H}
%\setcounter{secnumdepth}{0}

% Multi-column tables
\usepackage{multirow,tabularx}

% hyperlinks:
\ifthenelse{\isundefined{\hypersetup}}{
  \usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}
  \urlstyle{same} % normal text font (alternatives: tt, rm, sf)
}{}
\hypersetup{
  pdftitle={Visual Odometry for Autonomous Underwater Vehicles},
}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% Degrees
\newcommand{\degree}{\ensuremath{^\circ}}

\begin{document}

% paper title
\title{Visual Odometry for Autonomous Underwater Vehicles}

% author names and affiliations
% \author{\IEEEauthorblockN{Stephan Wirth}
% \IEEEauthorblockA{System, Robotics and Vision Group\\
% Universitat de les Illes Balears\\
% Palma de Mallorca, Spain\\
% Email: stephan.wirth@uib.es}
% \and
% \IEEEauthorblockN{Pep Lluis Negre Carrasco}
% \IEEEauthorblockA{System, Robotics and Vision Group\\
% Universitat de les Illes Balears\\
% Palma de Mallorca, Spain\\
% Email: pl.negre@uib.cat} 
% \and
% \IEEEauthorblockN{Gabriel Oliver Codina}
% \IEEEauthorblockA{System, Robotics and Vision Group\\
% Universitat de les Illes Balears\\
% Palma de Mallorca, Spain\\
% Email: goliver@uib.es}}

\author{\IEEEauthorblockN{
Stephan Wirth, 
Pep Lluis Negre Carrasco, 
Gabriel Oliver Codina}
\IEEEauthorblockA{
Departament de Matemàtiques i Informàtica, 
Universitat de les Illes Balears\\
Cra. Valldemossa, km. 7,5 07122 Palma de Mallorca (Spain)\\
Email: \{stephan.wirth, pl.negre, goliver\}@uib.es}}

% make the title area
\maketitle


\begin{abstract}
Vision based motion estimation algorithms are widely used in ground-based and aerial robotics. Combined with inertial measurement units, they have proven to be a precise and low-cost sensor for velocity and pose estimation. In this paper we show that stereo vision based odometry can be used by autonomous underwater vehicles that navigate close to the seabed for velocity and incremental pose estimation in small areas. We present the integration of two different stereo visual odometry algorithms into an AUV and experiments carried out in laboratory and harbour conditions comparing vision based pose estimates with ground truth.
\end{abstract}


\section{Introduction 
  \label{introduction}
}

The latest advances in Autonomous Underwater Vehicles (AUVs) demand accurate estimations of the vehicle's pose and velocity. The tasks to be performed by such robots are increasingly complex and, in most cases it is needed to precisely position the underwater vehicle to a certain location in order to execute the desired task. Navigation systems based on wirelessly transmitted data, such as the Global Positioning System (GPS) or mobile network positioning, can not be used in underwater environments. One can overcome this issue by introducing artificial landmarks. However the cost and effort for this is high and for small missions it is preferred to rely on positioning using self-contained sensors. One of the most popular positioning sensors for underwater vehicles is the Doppler Velocity Log (DVL) which provides precise velocity and altitude updates. However, it is an expensive device that cannot be integrated in many underwater vehicles.

Visual odometry is the process of estimating a vehicle's 3D pose using only visual images. This technique is becoming popular in AUVs for navigation, station keeping and the provision of feedback information for manipulation. Visual odometry output is often fused with Inertial Measurement Units (IMU) to provide a cheaper alternative to DVLs \cite{Hildebrandt2010,Dunbabin2004}. However, the limitations of underwater vision are widely known, and its performance depends on many factors such as visibility, lighting and distortion resulting from varying refractive indices.

In this paper we present the integration of two existing stereo visual odometry algorithms into an AUV, the experiments carried out in laboratory and harbour environments, as well as the evaluation method to compare the performance of both algorithms.

First, the paper explains related works on visual odometry. Section \ref{visual-odometry-systems} describes the general concepts of visual odometry systems and presents a brief overview of the two analyzed algorithms, followed by the explanation of the evaluation method in Section \ref{experiments} and experiments in different evironments. Finally, the conclusion of the study and future perspectives are presented.

\section{Related Work 
  \label{related-work}
}

The basic visual odometry algorithm pipeline \cite{Moravec1980} consists of the following steps: First, keypoints (landmarks) are identified in each camera frame and feature descriptors for these points are extracted. Then, the depth for every landmark is estimated using stereo, structure from motion or a separate depth camera. Subsequently, features are matched across time frames and the rigid-body transformation that best aligns the features between frames is estimated. The result of this process is an estimation of camera motion between frames and therefore it is necessary to integrate this data over time to obtain the vehicle's absolute position and orientation.

Many implementations of visual odometers for AUVs use this pipeline and adapt every stage (such us keypoint detector type and feature matching method) to their needs. Common feature detectors used for real-time visual odometry include Harris corners \cite{Harris1988,Nister2006}, FAST \cite{Rosten2006,Huang2011} or SIFT \cite{Lowe2004,Botelho2009} features. Wide range of approaches are involved in the process of matching feature across frames, \cite{Geiger2011} propose a circle match between stereo image pairs of two consecutive frames and uses non-minima/maxima-suppression techniques \cite{Geiger2010} to reduce the number of correspondences. RANSAC-based methods \cite{Nister2004,Johnson2008} and graph-based consistency algorithms \cite{Howard2008} have also been proven to robustly match features across frames.

Previous to the motion estimation stage, keypoint bucketing \cite{Zhang1995} has been shown to help reduce the inlier reprojection errors. Finally, the motion estimation process can be solved through different methods. Using a closed-form solution to the least-squares problem of absolute orientation \cite{Horn1987}, it is possible to directly minimize the Euclidean distance between matched features in order to compute the rigid-body transformation of the camera frame \cite{Huang2011}. Several ground-based visual odometers do not use 3D distances but implement the method of minimizing the pixel reprojection error \cite{Howard2008,Geiger2011,Huan2011}. 

Other localization algorithms are for AUVs are based on mosaics or structured environments and therefore assume prior knowledge of the area to explore \cite{Garcia2001,Gracias2003,Carreras2003}.

Many of the techniques and algorithms described in this section are primary focused on ground and aerial vehicles and most of them have not been tested in underwater environments. In this field, the complexity of the visual odometry task increases due to the dynamic light conditions and decreasing visibility with depth and turbidity. Our focus is primarily on integrating and validating two different stereo-based visual odometry approaches in such environments.

\section{Visual Odometry Systems
  \label{visual-odometry-systems}
}
A visual odometry system consists of one or more cameras, a processing unit, and the required algorithms that process incoming images.
Algorithms for visual odometry - as opposed to full SLAM algorithms - focus on fast frame-to-frame motion estimates without keeping a large history for loop closing. The emphasis lies on accurate measurements at high frequencies. Systems that use just one camera need translational movement for 3D motion estimation and all measurements have to be scaled by an unknown factor to be on a metric scale. Using a calibrated stereo camera overcomes both of the aforementioned problems as 3D coordinates of matched points in a single left/right image pair can be computed by triangulation. We therefore focus our work on visual odometry systems that use stereo cameras.

The two publicly available algorithms we compare are \emph{libviso2} \cite{Geiger2011} and \emph{fovis} \cite{Huang2011}. The former has been successfully used on cars, and the latter has proven to work for micro aerial vehicles.

We created wrappers for both libraries\footnote{See \url{http://www.ros.org/wiki/viso2_ros} and \url{http://www.ros.org/wiki/fovis_ros} for source code and documentation.} to integrate them into the Robot Operating System ROS \cite{Quigley2009}. This simplifies the integration in different robotic vehicles as well as comparison with other integrated sensors.


Both algorithms have very similar processing pipelines containing feature detection, filtering, matching, and motion estimation. Table \ref{odometry-comparison} summarizes the steps of the algorithms, outlining similarities and differences. To limit the extent of this paper, the interested reader is referred to the original papers for a deeper understanding and a more detailed description of the algorithms. In the following we give a brief overview of both algorithms.

\subsection{libviso2
  \label{libviso2}
}

The features used by libviso2 are simple blob and corner detector masks, resulting in a large amount of interest points of four different classes. Non-minima-/non-maxima-suppression is used for initial feature reduction. In this step, two different thresholds for minima/maxima selection are used resulting in two sets of sparse strong features and dense less strong features. This allows for a multi-stage matching coarse to fine using matches of the strong features to limit the search range for the weaker ones. The used descriptor contains only 16 values of sobel filter responses distributed inside an 11x11 pixel window. The similarity measure for two features is the sum of absolute differences of their descriptors. The computation of this measure is sped up using SIMD instructions.

The matching of features follows the following order: current left image to current right image, current right image to previous right image, previous right image to previous left image and previous left image to current right image. If the target feature of the last matching is the same as the source feature of the first matching the match is classified as valid. Apart from the aforementioned search range limitation, possibly wrong matches are discarded using a neighborhood support heuristic.
From the obtained matches a subset is drawn that is distributed over the whole image by dividing the image in a grid of subimages and drawing a fixed maximum number of matches from each cell.

Subsequently, 3D coordinates of the features in the previous image pair are calculated through triangulation. Gauss-Newton minimization of the reprojection error of these 3D points onto the current left and right images leads to the rigid transformation from the previous image's camera pose to the current camera pose. The motion estimation procedure is wrapped in a RANSAC \cite{Fischler1981} scheme to get rid of outliers.


\subsection{fovis
  \label{fovis}
}

fovis uses the FAST feature detector \cite{Rosten2005FPA,Rosten2006} with an adaptive threshold on three Gaussian pyramid levels. The number of features is reduced taking the best features of each cell of a regular grid. The feature descriptor is an intensity normalized 9x9 pixel window centered on the feature location. The bottom right pixel is left out to get better memory alignment for rapid similarity measure computation using SIMD instructions.

Features are matched from the current left to the left reference frame. The right frames are used for disparity lookup only. Inlier/outlier classification is done using the concept of a maximally consistent clique, i.e. matches are sequentially added while the relation of euclidean distance to other matched features does not change from the reference to the current frames. For motion estimation, the reprojection error from 3D positions of current features to the reference left frame as well as from 3D positions of the reference features to the current left frame is minimized using the Gauss-Newton method. If the reprojection error of a certain match exceeds a threshold, that match is discarded. The motion estimate is refined using only those matches that survived this process.

Apart from the used feature detectors, an important difference of the two algorithms is the selection of the reference frame for motion computation. libviso2 does not have automatic selection of the reference frame and replaces it in each iteration. In contrast, fovis only replaces the reference frame if the number of inlier matches drops below a given threshold. The purpose is to reduce drift that could arise from sensor noise or bad calibration.

Both algorithms use feature descriptors that are not invariant to rotation or scale changes. The frequency of both algorithms has therefore to be high to cope with these movements.

\begin{table*}[ht]
  \caption{Comparison of the two visual odometry algorithms.}
  \label{odometry-comparison}
  \begin{tabularx}{\linewidth}{l|X|X}
    & \textbf{libviso2} & \textbf{fovis}  \\
    \hline
    Feature Scales                  & 1 (reduced) level & 3 gaussian pyramid levels \\[3pt]
    Feature Detection               & 5x5 blob and corner masks & FAST with adaptive threshold  \\[3pt]
    Initial Feature Reduction       & non-minimum-/non-maximum-suppression & grid filter \\[3pt]
    Descriptor                      & 16 Sobel filter responses & 9x9 pixel window, intensity normalized \\[3pt]
    Subpixel Position Refinement    & parabolic fitting & ESM \\[3pt]
    \hline
    Matching Search Space Limitation & two-step candidate reduction & initial rotation estimate (Mei et al.) + search window \\[3pt]
    Matching Images                 & circlular match & left to right \& left to previous left \\[3pt]
    Match Reduction                 & grid filter & none \\[3pt]
    \hline
    Initial Outlier Rejection       & 2D neighborhood support & none \\[3pt]
    Outlier Classification          & iteratively during motion estimation (RANSAC) & maximally consistent clique \\[3pt]
    Minimized Reprojection Error    & previous 3D to current left and right image & previous 3D to current left and current 3D to previous left image \\[3pt]
    \hline
    Motion Filtering                & Kalman & none (external) \\[3pt]
    Keyframe Selection              & none (external) & automatic, based on number of matches \\[3pt]
  \end{tabularx}
\end{table*}


\section{Experiments
  \label{experiments}
}

The experiments carried out use data gathered during the TRIDENT project\footnote{See \url{http://www.irs.uji.es/trident/}} both in laboratory and harbour conditions using the Girona500 AUV \cite{Ribas2012} as vehicle (Figure \ref{g500}). This vehicle has been designed by the Universitat de Girona and, without change, does not have stereo vision system, but allows the mounting of external systems on the bottom payload area. This featured is used to attach the stereo vision unit developed by our Group in the Universitat de les Illes Balears. This unit is called Fugu-Flexible, or Fugu-F to shorten, which is basically composed of two stereo rigs and a computer system. Each module, hardware and cameras, is placed in an independent sealed case rated for up to 100 meters depth. The main Fugu-F hardware specifications are:

\begin{itemize}
  \item Two Point Grey Bumblebee2 stereo cameras, one with a focal distance of 3.8mm (66\degree HFOV), and the other with 2.5mm (97\degree HFOV). % HFOV in water?
  \item A mini-ITX motherboard from Jetway-NF98-QM57 Express Chipset with an Intel i5 processor at 2.33GHz with 4 cores.
  \item A PCI express card with two firewire ports from Chronos to connect the cameras to the motherboard.
\end{itemize}

\begin{figure}
  \noindent\makebox[\linewidth][c]{\includegraphics[width=1.000\linewidth]{images/G500-Experiments.jpg}}
  \caption{The Girona 500 AUV during laboratory and harbour experiments. The white box in the front of the lower part is the stereo camera. The cylinder right next to it contains the image processing unit.}
  \label{g500}
\end{figure}

%\begin{figure}
%  \noindent\makebox[\linewidth][c]{\includegraphics[width=0.7\linewidth]{images/fugu-f.jpg}}
%  \caption{Fugu-F: processing unit watertight cylinder with the two stereo rigs}
%  \label{fugu-f}
%\end{figure}

Figure \ref{sample-images} shows some example images from the laboratory and harbour enviroments. Ground truth for the laboratory experiment is extracted by matching each image that has been captured against the known image that is printed on the floor of the test pool. As the size of the print is known, 6 DOF camera poses can be computed minimizing reprojection errors of matched features.

\begin{figure}
  \noindent\makebox[\linewidth][c]{\includegraphics[width=1.000\linewidth]{images/sample_images.jpg}}
  \caption{Sample images from the sequences during the laboratory (top row) and harbour (bottom row) experiments.}
  \label{sample-images}
\end{figure}

For the harbour experiment, the determination of ground truth is more difficult, as no external sensors have been used and positions of natural landmarks are not known. One aim of the project was the offline construction of a consistent seabed mosaic (see \cite{Ferrer2007}). The computation of this mosaic includes global optimization of all camera poses. The resulting trajectory does not suffer from drift and is therefore chosen as our ground truth reference.

In the harbour experiment, a DVL and an AHS have been used as navigation sensors to let the vehicle follow a previously defined trajectory autonomously. In the laboratory experiments, the vehicle has been remotely controlled by a human operator. Table \ref{experiment-details} summarizes the characteristics of the experiments.

\begin{table}[!t]
  \renewcommand{\arraystretch}{1.3}
  \caption{Experiment details}
  \label{experiment-details}
  \centering
  \begin{tabular}{lcc}
    & \textbf{CIRS Lab} & \textbf{Roses Harbour}  \\ \hline
    Total Trajectory Length (m)     & 47.54 & 90.48  \\
    Average Velocity (m/s)          & 0.15 & 0.12  \\
    Average Altitude (m)            & 1.59 & 1.34  \\
    Average Depth (m)               & 2.98 & 1.46  \\
  \end{tabular}
\end{table}

The AUV was configured to not control pitch, roll and sway. Motions in these degrees of freedom are therefore rather small and we cannot compute reasonable error measures for them.

\subsection{Evaluation Method
  \label{evaluation-method}
}

As visual odometry suffers from drift, comparing whole trajectories, i.e. poses in time directly to ground truth is not very meaningful. Instead, we subdivide the paths into small pieces and compare velocities for each piece to the matching piece in our ground truth. This method is widely used in visual odometry evaluation \cite{Kitti2013} and depends clearly on the size of the subpaths. However, since it is used to compare different visual odometers, the important thing here is to be coherent with the size of the pieces for all algorithms.

\subsection{Results
  \label{results}
}

A comparison of linear velocity estimates to ground truth can be found in Figure \ref{error-plots-viso2} for libviso2 and \ref{error-plots-fovis} for fovis. Upper graphics refer to experiments in laboratory while the ones of the bottom refer to harbour tests. Linear velocity is shown in left graphics for both odometry estimation (blue) and ground truth (green), together with its error drawn in red. Finally, on right plot, a comparison of angular velocity for odometry estimation (blue) and grund truth (green) is also presented. No differences can be highlighted comparing angular velocity error for libviso2 and fovis in both the laboratory and harbour conditions. However, when linear velocities are examined, fovis presents better performance when executed in laboratory enviroment.

These results are summarized in Table \ref{results-errors} for laboratory and harbour experiments. The translation column indicates the mean average translation error that suffers the odometer in percent, which is the number of meters diverted for every 100 meters of trajectory. Furthermore, the rotation column indicates how many degrees the odometer is deviated per meter, relative to ground truth.

The results presented in Table \ref{results-errors} demostrates that libviso2 has similar mean translation error for both enviroments, but differs significantly in rotation error. By refering to Table \ref{results-runtime}, the mean number of inliers per frame is larger in the laboratory than in the harbour (where environmental conditions are worse). It means libviso2 presents a strong performance in the linear motion estimation when the number of inliers is reduced.

Results for fovis odometer are substantially different in translation error comparing laboratory and harbour conditions. By examining the number of inliers of the two odometers shown in Table \ref{results-runtime} it is easy to see that fovis is significantly less robust in harbour enviroments where the quality of the images is lower (see Figure \ref{sample-images}). Furthermore, unlike libviso2, fovis presents a better performance when estimating the angular motion in harbour conditions. This improvement over libviso2 is due to the method of keyframe selection based on the number of matches introduced by fovis. This technique has a positive impact when the vehicle experiences pure rotational movements, as it can keep the reference frame for longer and therefore reduce the drift significally.

The runtime column shown in Table \ref{results-runtime} indicates the mean algorithm execution time per frame. For both libviso2 and fovis odometers the runtime is clearly related to the number of inliers to be processed by the motion estimation algorithm, thus large number of inliers increases processing time.

\begin{figure*}
  \begin{center}
    \subfloat[]{
      \begin{tabular}{c}
        \includegraphics[width=0.45\linewidth]{aux/viso2-cirs-timefixed-run3-vt.pdf}\\
        \includegraphics[width=0.45\linewidth]{aux/viso2-roses-vt.pdf}
      \end{tabular}
    }
    \subfloat[]{
      \begin{tabular}{c}
        \includegraphics[width=0.45\linewidth]{aux/viso2-cirs-timefixed-run3-vyaw-err.pdf}\\
        \includegraphics[width=0.45\linewidth]{aux/viso2-roses-vyaw-err.pdf}
      \end{tabular}
    }
  \end{center}
  \caption{Linear (a) and angular (b) velocity estimates compared to ground truth for the laboratory (top) and the harbour (bottom) experiments for libviso2.}
  \label{error-plots-viso2}
\end{figure*}

\begin{figure*}[!h]
  \begin{center}
    \subfloat[]{
      \begin{tabular}{c}
        \includegraphics[width=0.45\linewidth]{aux/fovis-cirs-timefixed-run3-vt.pdf}\\
        \includegraphics[width=0.45\linewidth]{aux/fovis-roses-vt.pdf}
      \end{tabular}
    }
    \subfloat[]{
      \begin{tabular}{c}
        \includegraphics[width=0.45\linewidth]{aux/fovis-cirs-timefixed-run3-vyaw-err.pdf}\\
        \includegraphics[width=0.45\linewidth]{aux/fovis-roses-vyaw-err.pdf}
      \end{tabular}
    }
  \end{center}
  \caption{Linear (a) and angular (b) velocity estimates compared to ground truth for the laboratory (top) and the harbour (bottom) experiments for fovis.}
  \label{error-plots-fovis}
\end{figure*}

\begin{table}[!t]
  \renewcommand{\arraystretch}{1.3}
  \caption{Comparison of translational and rotational errors for laboratory and harbour experiments.}
  \label{results-errors}
  \centering
  \begin{tabular}{|l|l|c|c|} \hline
    \textbf{Enviroment} & \textbf{Algorithm} & \textbf{Translation} & \textbf{Rotation}\\ \hline
    \multirow{2}{*}{Laboratory} & libviso2 & 0.978\% & 0.001739 [deg/m] \\
                                & fovis    & 0.601\% & 0.006311 [deg/m] \\ \hline
    \multirow{2}{*}{Harbour}    & libviso2 & 0.833\% & 0.007913 [deg/m] \\
                                & fovis    & 1.049\% & 0.003122 [deg/m] \\ \hline
  \end{tabular} 
\end{table}

\begin{table}[!t]
  \renewcommand{\arraystretch}{1.3}
  \caption{Comparison of number of inliear and runtime for laboratory and harbour experiments.}
  \label{results-runtime}
  \centering
  \begin{tabular}{|l|l|c|c|} \hline
    \textbf{Enviroment} & \textbf{Algorithm} & \textbf{Num. inliers} & \textbf{Runtime}\\ \hline
    \multirow{2}{*}{Laboratory} & libviso2 & 366 & 0.106 [s] \\
                                & fovis    & 500 & 0.246 [s] \\ \hline
    \multirow{2}{*}{Harbour}    & libviso2 & 137 & 0.086 [s] \\
                                & fovis    & 64 & 0.058 [s] \\ \hline
  \end{tabular} 
\end{table}

\section{Conclusion
  \label{conclusion}
}

Visual odometry is a method for motion estimation that gives good results using a low-cost sensor in reasonable conditions. We presented open source wrappers for two publicly available visual odometry algorithms to the community that ease the integration into existing robotic platforms.
We have shown that these algorithms can be used in an underwater environment if both the visibility conditions and the appearance of the sea floor result in images with sufficient texture. Like all incremental motion estimation methods, visual odometry suffers from drift and has to be combined with other sensors to get precise long-term position estimates. Apart from that, failure might occur in situations with insufficient texture, motion blur, or bad visibility.

Despite the mentioned drawbacks we have shown that a visual odometer using a downward looking stereo camera can be a valuable sensor for underwater vehicles giving good estimates for linear movement and rotation about the vehicle's vertical axis. Comparing the performance of libviso2 and fovis in underwater enviroments, libviso2 is preferible when the vehicle operates in real marine waters with critical lighting conditions and slightly textured seabed. The robustness of this algorithm against poor visibility contrasts with the fact of having a worse rotation estimation when compared with fovis in harbour experiments.

A possible improvement for the angular motion estimation in libviso2 could be the implementation of the reference frame selection technique developed by fovis algorithm. This method could substancialy reduce the drift in both linear and angular estimates.

\section{Acknowledgment
  \label{acknowledgment}
}

This work is partially supported by the Spanish Ministry of Economy and Competitiveness under contract DPI2011-27977-C03- 02 (TRITON Project), FEDER Fundings and by the European Commissions FP7 under grant agreement 248497 (TRIDENT Project).

% references section
\bibliographystyle{IEEEtran}
\bibliography{references}

% that's all folks
\end{document}


